const _astro_dataLayerContent = [["Map",1,2,9,10,431,432],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.16.5","content-config-digest","9161f7efe46c811f","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://basementos.com\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{\"/businesscards.html\":\"/businesscards/\"},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","devlog",["Map",11,12,37,38,60,61,82,83,103,104,125,126,149,150,174,175,196,197,217,218,239,240,262,263,283,284,305,306,338,339,357,358,388,389,410,411],"2025-07-13-vrchat-playerdata-persistence",{id:11,data:13,body:17,filePath:18,digest:19,rendered:20,legacyId:36},{title:14,date:15,type:16},"VRChat PlayerData Persistence",["Date","2025-07-13T00:00:00.000Z"],"milestone","Finally got VRChat's [PlayerData API](https://docs.vrchat.com/docs/player-persistence) working! Visit counts now persist between sessions. This took way longer than expected because of UdonSharp's limitations.\r\n\r\nCan't use Dictionary or List in UdonSharp, so I'm tracking players with parallel arrays. Not elegant, but it works. Successfully tested with 3 visits - data persists across world reloads.\r\n\r\nThe 11-hour debugging marathon was worth it. This is the foundation for the entire achievement system.\r\n\r\n[![PlayerData persistence working](/Manual Change Logs and Images/images/July 2025/Persistence Working.jpg)](/Manual Change Logs and Images/images/July 2025/Persistence Working.jpg)\r\n\r\n[![Successfully tracking 3 visits](/Manual Change Logs and Images/images/July 2025/tracking 3 visits.png)](/Manual Change Logs and Images/images/July 2025/tracking 3 visits.png)","src/content/devlog/2025-07-13-vrchat-playerdata-persistence.md","a07634f4839deccf",{html:21,metadata:22},"<p>Finally got VRChat’s <a href=\"https://docs.vrchat.com/docs/player-persistence\">PlayerData API</a> working! Visit counts now persist between sessions. This took way longer than expected because of UdonSharp’s limitations.</p>\n<p>Can’t use Dictionary or List in UdonSharp, so I’m tracking players with parallel arrays. Not elegant, but it works. Successfully tested with 3 visits - data persists across world reloads.</p>\n<p>The 11-hour debugging marathon was worth it. This is the foundation for the entire achievement system.</p>\n<p>[![PlayerData persistence working](/Manual Change Logs and Images/images/July 2025/Persistence Working.jpg)](/Manual Change Logs and Images/images/July 2025/Persistence Working.jpg)</p>\n<p>[![Successfully tracking 3 visits](/Manual Change Logs and Images/images/July 2025/tracking 3 visits.png)](/Manual Change Logs and Images/images/July 2025/tracking 3 visits.png)</p>",{headings:23,localImagePaths:24,remoteImagePaths:25,frontmatter:26,imagePaths:35},[],[],[],{title:14,date:27,tags:28,type:16},["Date","2025-07-13T00:00:00.000Z"],[29,30,31,32,33,34],"MILESTONE","persistence","vrchat","udonsharp","VRChat PlayerData API","UdonSharp",[],"2025-07-13-vrchat-playerdata-persistence.md","2025-07-10-first-achievement-unlocked",{id:37,data:39,body:43,filePath:44,digest:45,rendered:46,legacyId:59},{title:40,date:41,type:42},"First Achievement Unlocked",["Date","2025-07-10T00:00:00.000Z"],"update","First achievement notification popped in-world today! \"First Time Visitor\" - the notification banner slides in from the right with that Xbox 360 bloop sound. Feels exactly like it did on the 360.\r\n\r\nThe FIFO queue, animation timing, and sound cues all working together. This is the moment I've been building toward since starting this project.\r\n\r\n[![First achievement notification in production](/Manual Change Logs and Images/images/July 2025/First Welcome Message in prod 7 10 25.png)](/Manual Change Logs and Images/images/July 2025/First Welcome Message in prod 7 10 25.png)\r\n\r\n[![First achievement unlocked screenshot](/Manual Change Logs and Images/images/July 2025/first achievement unlocked 7 10 25.png)](/Manual Change Logs and Images/images/July 2025/first achievement unlocked 7 10 25.png)","src/content/devlog/2025-07-10-first-achievement-unlocked.md","c0617d8c686041f6",{html:47,metadata:48},"<p>First achievement notification popped in-world today! “First Time Visitor” - the notification banner slides in from the right with that Xbox 360 bloop sound. Feels exactly like it did on the 360.</p>\n<p>The FIFO queue, animation timing, and sound cues all working together. This is the moment I’ve been building toward since starting this project.</p>\n<p>[![First achievement notification in production](/Manual Change Logs and Images/images/July 2025/First Welcome Message in prod 7 10 25.png)](/Manual Change Logs and Images/images/July 2025/First Welcome Message in prod 7 10 25.png)</p>\n<p>[![First achievement unlocked screenshot](/Manual Change Logs and Images/images/July 2025/first achievement unlocked 7 10 25.png)](/Manual Change Logs and Images/images/July 2025/first achievement unlocked 7 10 25.png)</p>",{headings:49,localImagePaths:50,remoteImagePaths:51,frontmatter:52,imagePaths:58},[],[],[],{title:40,date:53,tags:54},["Date","2025-07-10T00:00:00.000Z"],[55,56,57],"launch","achievements","ui",[],"2025-07-10-first-achievement-unlocked.md","2025-07-19-multi-tv-broadcasting-system",{id:60,data:62,body:65,filePath:66,digest:67,rendered:68,legacyId:81},{title:63,date:64,type:42},"Multi-TV Broadcasting System",["Date","2025-07-19T00:00:00.000Z"],"Got notifications working on all 3 TVs simultaneously! The NotificationEventHub broadcasts to each display independently, so everyone in the basement sees achievements pop regardless of which room they're in.\r\n\r\nEach TV maintains its own FIFO queue and animation timing. Had to be careful with the ProTV prefab integration - it uses a different Canvas setup than standard UI.\r\n\r\n[![Notifications working on all 3 TVs](/Manual Change Logs and Images/images/July 2025/Notifications working on 3 Tvs!!!.jpg)](/Manual Change Logs and Images/images/July 2025/Notifications working on 3 Tvs!!!.jpg)\r\n\r\n[![Multi-TV setup in basement](/Manual Change Logs and Images/images/July 2025/Multiple TVs.jpg)](/Manual Change Logs and Images/images/July 2025/Multiple TVs.jpg)","src/content/devlog/2025-07-19-multi-tv-broadcasting-system.md","4fd7d8b7fc425347",{html:69,metadata:70},"<p>Got notifications working on all 3 TVs simultaneously! The NotificationEventHub broadcasts to each display independently, so everyone in the basement sees achievements pop regardless of which room they’re in.</p>\n<p>Each TV maintains its own FIFO queue and animation timing. Had to be careful with the ProTV prefab integration - it uses a different Canvas setup than standard UI.</p>\n<p>[![Notifications working on all 3 TVs](/Manual Change Logs and Images/images/July 2025/Notifications working on 3 Tvs!!!.jpg)](/Manual Change Logs and Images/images/July 2025/Notifications working on 3 Tvs!!!.jpg)</p>\n<p>[![Multi-TV setup in basement](/Manual Change Logs and Images/images/July 2025/Multiple TVs.jpg)](/Manual Change Logs and Images/images/July 2025/Multiple TVs.jpg)</p>",{headings:71,localImagePaths:72,remoteImagePaths:73,frontmatter:74,imagePaths:80},[],[],[],{title:63,date:75,tags:76},["Date","2025-07-19T00:00:00.000Z"],[77,57,78,34,79],"notifications","networking","ProTV",[],"2025-07-19-multi-tv-broadcasting-system.md","2025-10-20-achievement-system-overhaul",{id:82,data:84,body:87,filePath:88,digest:89,rendered:90,legacyId:102},{title:85,date:86,type:16},"Achievement System Overhaul",["Date","2025-10-20T00:00:00.000Z"],"Finally finished the Xbox 360-style achievement system! 19 achievements worth 420G implemented out of 1,000G leaves plenty of room for expansion with future ideas. This matches the original Xbox 360 gamer score point structure. The notifications pop up just like they did on the 360 - that satisfying sound effect and the animated banner!\r\n\r\nUsing VRChat's [PlayerData API](https://docs.vrchat.com/docs/player-persistence) for persistence. This was tricky because you can't use fancy C# features in UdonSharp - no `List<T>`, no Dictionary, no LINQ. Everything's done with arrays and careful indexing.\r\n\r\nThe FIFO queue for notifications took a few iterations. Originally had a priority system but it felt weird when achievements popped up out of order. The chronological approach matches the \"basement live feed\" vibe I was going for.","src/content/devlog/2025-10-20-achievement-system-overhaul.md","dc1e5aee603927c9",{html:91,metadata:92},"<p>Finally finished the Xbox 360-style achievement system! 19 achievements worth 420G implemented out of 1,000G leaves plenty of room for expansion with future ideas. This matches the original Xbox 360 gamer score point structure. The notifications pop up just like they did on the 360 - that satisfying sound effect and the animated banner!</p>\n<p>Using VRChat’s <a href=\"https://docs.vrchat.com/docs/player-persistence\">PlayerData API</a> for persistence. This was tricky because you can’t use fancy C# features in UdonSharp - no <code>List&#x3C;T></code>, no Dictionary, no LINQ. Everything’s done with arrays and careful indexing.</p>\n<p>The FIFO queue for notifications took a few iterations. Originally had a priority system but it felt weird when achievements popped up out of order. The chronological approach matches the “basement live feed” vibe I was going for.</p>",{headings:93,localImagePaths:94,remoteImagePaths:95,frontmatter:96,imagePaths:101},[],[],[],{title:85,date:97,tags:98,type:16},["Date","2025-10-20T00:00:00.000Z"],[29,56,30,99,33,34,100],"xbox","TextMeshPro",[],"2025-10-20-achievement-system-overhaul.md","2025-07-26-achievement-icon-design",{id:103,data:105,body:108,filePath:109,digest:110,rendered:111,legacyId:124},{title:106,date:107,type:42},"Achievement Icon Design",["Date","2025-07-26T00:00:00.000Z"],"Finished designing custom icons for all 19 achievements using Photopea.com. Referenced actual Xbox 360 achievement art to match that 2000s gaming aesthetic.\r\n\r\n[![Achievement icon sketches](/Manual Change Logs and Images/images/360 Icons/Photo Jul 26 2025, 9 17 32 PM.png)](/Manual Change Logs and Images/images/360 Icons/Photo Jul 26 2025, 9 17 32 PM.png)\r\n\r\n[![Custom achievement icons in Unity](/Manual Change Logs and Images/images/July 2025/Custom Icons.jpg)](/Manual Change Logs and Images/images/July 2025/Custom Icons.jpg)","src/content/devlog/2025-07-26-achievement-icon-design.md","3f25f4c4abca6e5b",{html:112,metadata:113},"<p>Finished designing custom icons for all 19 achievements using Photopea.com. Referenced actual Xbox 360 achievement art to match that 2000s gaming aesthetic.</p>\n<p>[![Achievement icon sketches](/Manual Change Logs and Images/images/360 Icons/Photo Jul 26 2025, 9 17 32 PM.png)](/Manual Change Logs and Images/images/360 Icons/Photo Jul 26 2025, 9 17 32 PM.png)</p>\n<p>[![Custom achievement icons in Unity](/Manual Change Logs and Images/images/July 2025/Custom Icons.jpg)](/Manual Change Logs and Images/images/July 2025/Custom Icons.jpg)</p>",{headings:114,localImagePaths:115,remoteImagePaths:116,frontmatter:117,imagePaths:123},[],[],[],{title:106,date:118,tags:119},["Date","2025-07-26T00:00:00.000Z"],[120,57,56,121,122],"art","Photopea","Unity",[],"2025-07-26-achievement-icon-design.md","2025-08-07-weather-system",{id:125,data:127,body:130,filePath:131,digest:132,rendered:133,legacyId:148},{title:128,date:129,type:42},"Weather System + Rain Shaders",["Date","2025-08-07T00:00:00.000Z"],"Integrated real-time weather from Fond du Lac, WI using a GitHub Pages JSON endpoint. The terminal displays current conditions and when it's actually raining outside, the basement windows show rain effects.\r\n\r\nRain shader source: [PLACEHOLDER - Please specify the rain shader source (Unity Asset Store, GitHub repo, custom made, etc.)]\r\n\r\n[![Thundery outbreaks weather condition](/Manual Change Logs and Images/images/August 2025/'Thunderyoutbreaksinnearby' Screenshot 2025-08-07 195825.jpg)](/Manual Change Logs and Images/images/August 2025/'Thunderyoutbreaksinnearby' Screenshot 2025-08-07 195825.jpg)\r\n\r\n[![Weather system working on terminal](/Manual Change Logs and Images/images/July 2025/working weather.png)](/Manual Change Logs and Images/images/July 2025/working weather.png)","src/content/devlog/2025-08-07-weather-system.md","57941ea9b929ee3f",{html:134,metadata:135},"<p>Integrated real-time weather from Fond du Lac, WI using a GitHub Pages JSON endpoint. The terminal displays current conditions and when it’s actually raining outside, the basement windows show rain effects.</p>\n<p>Rain shader source: [PLACEHOLDER - Please specify the rain shader source (Unity Asset Store, GitHub repo, custom made, etc.)]</p>\n<p>[![Thundery outbreaks weather condition](/Manual Change Logs and Images/images/August 2025/‘Thunderyoutbreaksinnearby’ Screenshot 2025-08-07 195825.jpg)](/Manual Change Logs and Images/images/August 2025/‘Thunderyoutbreaksinnearby’ Screenshot 2025-08-07 195825.jpg)</p>\n<p>[![Weather system working on terminal](/Manual Change Logs and Images/images/July 2025/working weather.png)](/Manual Change Logs and Images/images/July 2025/working weather.png)</p>",{headings:136,localImagePaths:137,remoteImagePaths:138,frontmatter:139,imagePaths:147},[],[],[],{title:128,date:140,tags:141},["Date","2025-08-07T00:00:00.000Z"],[142,143,144,145,146,34],"weather","api","shaders","GitHub Pages","VRCUrlLoader",[],"2025-08-07-weather-system.md","2025-11-07-claude-code-sprint",{id:149,data:151,body:154,filePath:155,digest:156,rendered:157,legacyId:173},{title:152,date:153,type:16},"Claude Code + Documentation Sprint",["Date","2025-11-07T00:00:00.000Z"],"Started my first session with [Claude Code](https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview) - this is a game changer. Instead of copying code snippets back and forth, Claude can directly read my project files, write code, and even help with git commits.\r\n\r\nSpent the session adding comprehensive XML docstrings to all major scripts. The AchievementTracker alone went from zero documentation to fully annotated with parameter descriptions and usage examples.\r\n\r\nAlso started setting up project automation - automatic UdonSharp validation before commits, organized GitHub issues with story points, and established milestones. Treating Claude like a jr developer is really showing how much leadership and project management coordination I have in this area.\r\n\r\n[![Adding XML docstrings with Claude Code](/Manual Change Logs and Images/images/Claude Code Jam Session November/Adding Doc Strings 2025-11-07 180219.png)](/Manual Change Logs and Images/images/Claude Code Jam Session November/Adding Doc Strings 2025-11-07 180219.png)\r\n\r\n[![First git upload with Claude Code](/Manual Change Logs and Images/images/Claude Code Jam Session November/first upload using git.png)](/Manual Change Logs and Images/images/Claude Code Jam Session November/first upload using git.png)\r\n\r\n[![GitHub issue organization](/Manual Change Logs and Images/images/Claude Code Jam Session November/organizing issues using github and roadmap.png)](/Manual Change Logs and Images/images/Claude Code Jam Session November/organizing issues using github and roadmap.png)","src/content/devlog/2025-11-07-claude-code-sprint.md","2e6c1a699ca16f11",{html:158,metadata:159},"<p>Started my first session with <a href=\"https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview\">Claude Code</a> - this is a game changer. Instead of copying code snippets back and forth, Claude can directly read my project files, write code, and even help with git commits.</p>\n<p>Spent the session adding comprehensive XML docstrings to all major scripts. The AchievementTracker alone went from zero documentation to fully annotated with parameter descriptions and usage examples.</p>\n<p>Also started setting up project automation - automatic UdonSharp validation before commits, organized GitHub issues with story points, and established milestones. Treating Claude like a jr developer is really showing how much leadership and project management coordination I have in this area.</p>\n<p>[![Adding XML docstrings with Claude Code](/Manual Change Logs and Images/images/Claude Code Jam Session November/Adding Doc Strings 2025-11-07 180219.png)](/Manual Change Logs and Images/images/Claude Code Jam Session November/Adding Doc Strings 2025-11-07 180219.png)</p>\n<p>[![First git upload with Claude Code](/Manual Change Logs and Images/images/Claude Code Jam Session November/first upload using git.png)](/Manual Change Logs and Images/images/Claude Code Jam Session November/first upload using git.png)</p>\n<p>[![GitHub issue organization](/Manual Change Logs and Images/images/Claude Code Jam Session November/organizing issues using github and roadmap.png)](/Manual Change Logs and Images/images/Claude Code Jam Session November/organizing issues using github and roadmap.png)</p>",{headings:160,localImagePaths:161,remoteImagePaths:162,frontmatter:163,imagePaths:172},[],[],[],{title:152,date:164,tags:165,type:16},["Date","2025-11-07T00:00:00.000Z"],[29,166,167,168,169,170,171],"documentation","automation","ai","Claude Code","GitHub","Git",[],"2025-11-07-claude-code-sprint.md","2025-11-29-symbolic-games-poc",{id:174,data:176,body:179,filePath:180,digest:181,rendered:182,legacyId:195},{title:177,date:178,type:42},"Symbolic Games POC",["Date","2025-11-29T00:00:00.000Z"],"Experimented with terminal-based games using unicode characters instead of sprites. Built breakout and pong prototypes that render using block characters.\r\n\r\nInteresting concept, but after testing found sprites are significantly less resource-intensive on Quest. Keeping the code for reference but won't ship in prod.\r\n\r\n[![Symbolic rendering engine working](/Manual Change Logs and Images/images/Claude Code Jam Session November/11-29-25 Symbolic Rendering Engine WORKING.png)](/Manual Change Logs and Images/images/Claude Code Jam Session November/11-29-25 Symbolic Rendering Engine WORKING.png)\r\n\r\n[![Symbolic breakout game](/Manual Change Logs and Images/images/Claude Code Jam Session November/Symbolic Breakout .png)](/Manual Change Logs and Images/images/Claude Code Jam Session November/Symbolic Breakout .png)","src/content/devlog/2025-11-29-symbolic-games-poc.md","a5ece17169a09b81",{html:183,metadata:184},"<p>Experimented with terminal-based games using unicode characters instead of sprites. Built breakout and pong prototypes that render using block characters.</p>\n<p>Interesting concept, but after testing found sprites are significantly less resource-intensive on Quest. Keeping the code for reference but won’t ship in prod.</p>\n<p>[![Symbolic rendering engine working](/Manual Change Logs and Images/images/Claude Code Jam Session November/11-29-25 Symbolic Rendering Engine WORKING.png)](/Manual Change Logs and Images/images/Claude Code Jam Session November/11-29-25 Symbolic Rendering Engine WORKING.png)</p>\n<p>[![Symbolic breakout game](/Manual Change Logs and Images/images/Claude Code Jam Session November/Symbolic Breakout .png)](/Manual Change Logs and Images/images/Claude Code Jam Session November/Symbolic Breakout .png)</p>",{headings:185,localImagePaths:186,remoteImagePaths:187,frontmatter:188,imagePaths:194},[],[],[],{title:177,date:189,tags:190},["Date","2025-11-29T00:00:00.000Z"],[191,192,193,100,34],"terminal","games","optimization",[],"2025-11-29-symbolic-games-poc.md","2025-08-10-world-launch-party",{id:196,data:198,body:201,filePath:202,digest:203,rendered:204,legacyId:216},{title:199,date:200,type:16},"World Launch Party",["Date","2025-08-10T00:00:00.000Z"],"Opened Lower Level 2.0 to the public! Had about 8 friends show up for the launch. Watching the achievement notifications pop as people joined was incredibly satisfying - exactly the vibe I was going for.\r\n\r\nEverything worked smoothly - notifications, persistence, the DOS terminal.\r\n\r\n[![Launch party screenshot with 8 friends](/Manual Change Logs and Images/images/August 2025/VRChat_2025-08-10_20-58-06.405_2560x1440 launch party.png)](/Manual Change Logs and Images/images/August 2025/VRChat_2025-08-10_20-58-06.405_2560x1440 launch party.png)\r\n\r\n[![Achievement notifications during launch](/Manual Change Logs and Images/images/August 2025/hangout verified.jpg)](/Manual Change Logs and Images/images/August 2025/hangout verified.jpg)","src/content/devlog/2025-08-10-world-launch-party.md","59190d5dc738795a",{html:205,metadata:206},"<p>Opened Lower Level 2.0 to the public! Had about 8 friends show up for the launch. Watching the achievement notifications pop as people joined was incredibly satisfying - exactly the vibe I was going for.</p>\n<p>Everything worked smoothly - notifications, persistence, the DOS terminal.</p>\n<p>[![Launch party screenshot with 8 friends](/Manual Change Logs and Images/images/August 2025/VRChat_2025-08-10_20-58-06.405_2560x1440 launch party.png)](/Manual Change Logs and Images/images/August 2025/VRChat_2025-08-10_20-58-06.405_2560x1440 launch party.png)</p>\n<p>[![Achievement notifications during launch](/Manual Change Logs and Images/images/August 2025/hangout verified.jpg)](/Manual Change Logs and Images/images/August 2025/hangout verified.jpg)</p>",{headings:207,localImagePaths:208,remoteImagePaths:209,frontmatter:210,imagePaths:215},[],[],[],{title:199,date:211,tags:212,type:16},["Date","2025-08-10T00:00:00.000Z"],[29,55,213,214],"social","VRChat",[],"2025-08-10-world-launch-party.md","2025-11-15-terminal-menu-system",{id:217,data:219,body:222,filePath:223,digest:224,rendered:225,legacyId:238},{title:220,date:221,type:42},"Terminal Menu System Complete",["Date","2025-11-15T00:00:00.000Z"],"In a development instance I ran a POC that replaces the original auto-cycling display with an actual interactive menu. Players could use their movement controls to navigate up/down through options and select with the interact button.\r\n\r\nHad to implement player immobilization when they're at the terminal - otherwise pressing up/down would move your avatar AND the cursor. Using [VRCStation](https://docs.vrchat.com/docs/vrcstation) for this also solves the \"walking away mid-interaction\" problem.\r\n\r\nAlso extracted the weather module into its own script. The terminal now pulls real-time weather data from my [GitHub Pages](https://pages.github.com/) endpoint and displays it in the header. When it's actually raining in Fond du Lac, you'll see rain in the basement too once I figure out how to re-bake the lighting with the shader enabled windows.","src/content/devlog/2025-11-15-terminal-menu-system.md","fe728161e4d4a812",{html:226,metadata:227},"<p>In a development instance I ran a POC that replaces the original auto-cycling display with an actual interactive menu. Players could use their movement controls to navigate up/down through options and select with the interact button.</p>\n<p>Had to implement player immobilization when they’re at the terminal - otherwise pressing up/down would move your avatar AND the cursor. Using <a href=\"https://docs.vrchat.com/docs/vrcstation\">VRCStation</a> for this also solves the “walking away mid-interaction” problem.</p>\n<p>Also extracted the weather module into its own script. The terminal now pulls real-time weather data from my <a href=\"https://pages.github.com/\">GitHub Pages</a> endpoint and displays it in the header. When it’s actually raining in Fond du Lac, you’ll see rain in the basement too once I figure out how to re-bake the lighting with the shader enabled windows.</p>",{headings:228,localImagePaths:229,remoteImagePaths:230,frontmatter:231,imagePaths:237},[],[],[],{title:220,date:232,tags:233},["Date","2025-11-15T00:00:00.000Z"],[191,234,142,235,34,236],"input","VRCStation","GitHub Pages API",[],"2025-11-15-terminal-menu-system.md","2025-12-06-closed-loop-system",{id:239,data:241,body:245,filePath:246,digest:247,rendered:248,legacyId:261},{title:242,date:243,type:16,description:244},"Full Closed-Loop Automation",["Date","2025-12-06T00:00:00.000Z"],"The moment the loop closed. AI writing code, compiling it, and testing it without human hands.","This is a big one. I've been working with <span class=\"term\" tabindex=\"0\">Claude Code<span class=\"tooltip\">An AI coding assistant by Anthropic that can read, write, and execute code autonomously</span></span> to build out the Basement OS kernel, and we finally cracked the automation problem.\r\n\r\n**The Problem:** With <span class=\"term\" tabindex=\"0\">UdonSharp<span class=\"tooltip\">A C# to Udon compiler that lets you write VRChat scripts in familiar C# syntax instead of visual programming</span></span>, AI can write perfect code that won't run. Unity needs to generate .asset files, attach them to GameObjects, and compile everything. My first two attempts failed because Claude would generate code with no way to test it. I was still the button-clicker.\r\n\r\n**The Solution:** <span class=\"term\" tabindex=\"0\">Unity MCP<span class=\"tooltip\">Model Context Protocol - a way for AI agents to communicate with Unity Editor directly</span></span> gave Claude hands. Now it does the full loop: write script → trigger compilation → check errors → attach to GameObjects → enter Play mode → verify. Zero human intervention.\r\n\r\n**The Learning:** This taught me that real automation isn't about speed - it's about eliminating the feedback loop. I went from \"human as button-clicker\" to \"human as architect.\" That's the leadership transfer I'm after. As [HumanLayer puts it](https://www.humanlayer.dev/blog/writing-a-good-claude-md), good AI tooling is about leveraging stateless functions correctly.\r\n\r\n**Why It Matters:** This pattern applies beyond VRChat. Any runtime environment (web apps, mobile, game engines) needs autonomous test → fix → verify loops. Companies pay for people who build these internal tools.","src/content/devlog/2025-12-06-closed-loop-system.md","a9eab8c393bf740a",{html:249,metadata:250},"<p>This is a big one. I’ve been working with <span class=\"term\" tabindex=\"0\">Claude Code<span class=\"tooltip\">An AI coding assistant by Anthropic that can read, write, and execute code autonomously</span></span> to build out the Basement OS kernel, and we finally cracked the automation problem.</p>\n<p><strong>The Problem:</strong> With <span class=\"term\" tabindex=\"0\">UdonSharp<span class=\"tooltip\">A C# to Udon compiler that lets you write VRChat scripts in familiar C# syntax instead of visual programming</span></span>, AI can write perfect code that won’t run. Unity needs to generate .asset files, attach them to GameObjects, and compile everything. My first two attempts failed because Claude would generate code with no way to test it. I was still the button-clicker.</p>\n<p><strong>The Solution:</strong> <span class=\"term\" tabindex=\"0\">Unity MCP<span class=\"tooltip\">Model Context Protocol - a way for AI agents to communicate with Unity Editor directly</span></span> gave Claude hands. Now it does the full loop: write script → trigger compilation → check errors → attach to GameObjects → enter Play mode → verify. Zero human intervention.</p>\n<p><strong>The Learning:</strong> This taught me that real automation isn’t about speed - it’s about eliminating the feedback loop. I went from “human as button-clicker” to “human as architect.” That’s the leadership transfer I’m after. As <a href=\"https://www.humanlayer.dev/blog/writing-a-good-claude-md\">HumanLayer puts it</a>, good AI tooling is about leveraging stateless functions correctly.</p>\n<p><strong>Why It Matters:</strong> This pattern applies beyond VRChat. Any runtime environment (web apps, mobile, game engines) needs autonomous test → fix → verify loops. Companies pay for people who build these internal tools.</p>",{headings:251,localImagePaths:252,remoteImagePaths:253,frontmatter:254,imagePaths:260},[],[],[],{title:242,date:255,type:16,tags:256,description:244},["Date","2025-12-06T00:00:00.000Z"],[29,257,167,168,258,169,259],"architecture","Unity MCP","Agent Swarm",[],"2025-12-06-closed-loop-system.md","2025-12-03-refactoring-claude-md",{id:262,data:264,body:267,filePath:268,digest:269,rendered:270,legacyId:282},{title:265,date:266,type:42},"Refactoring CLAUDE.md",["Date","2025-12-03T00:00:00.000Z"],"A coworker sent me [HumanLayer's guide to writing good CLAUDE.md files](https://www.humanlayer.dev/blog/writing-a-good-claude-md), and I couldn't help myself - had to try it immediately.\r\n\r\nOpened Claude Opus and fed it my entire 953-line CLAUDE.md for review, citing the HumanLayer article as the comparison benchmark. \"How does mine stack up against their recommendations?\"\r\n\r\nOpus came back with a refactor plan: too many instructions causing \"instruction-following decay,\" embedded code examples getting stale, mixed universal and task-specific rules. The solution? Modular structure - keep CLAUDE.md lean (~150 lines) and create reference documents in `Docs/Reference/` that Claude can pull when needed.\r\n\r\n[![Claude Opus refactor analysis showing before/after comparison](/images/2025/12/claude-md-refactor-opus-dec3.png)](/images/2025/12/claude-md-refactor-opus-dec3.png)\r\n\r\nThis is how I learn best. Read something interesting, apply it immediately while it's fresh. No analysis paralysis, just iteration. By the end of the session, I had a new doc structure that made every future Claude conversation more effective.\r\n\r\nMeta-learning: using AI to improve how you work with AI. That's leverage.","src/content/devlog/2025-12-03-refactoring-claude-md.md","bf14b08037e234bc",{html:271,metadata:272},"<p>A coworker sent me <a href=\"https://www.humanlayer.dev/blog/writing-a-good-claude-md\">HumanLayer’s guide to writing good CLAUDE.md files</a>, and I couldn’t help myself - had to try it immediately.</p>\n<p>Opened Claude Opus and fed it my entire 953-line CLAUDE.md for review, citing the HumanLayer article as the comparison benchmark. “How does mine stack up against their recommendations?”</p>\n<p>Opus came back with a refactor plan: too many instructions causing “instruction-following decay,” embedded code examples getting stale, mixed universal and task-specific rules. The solution? Modular structure - keep CLAUDE.md lean (~150 lines) and create reference documents in <code>Docs/Reference/</code> that Claude can pull when needed.</p>\n<p><a href=\"/images/2025/12/claude-md-refactor-opus-dec3.png\"><img src=\"/images/2025/12/claude-md-refactor-opus-dec3.png\" alt=\"Claude Opus refactor analysis showing before/after comparison\"></a></p>\n<p>This is how I learn best. Read something interesting, apply it immediately while it’s fresh. No analysis paralysis, just iteration. By the end of the session, I had a new doc structure that made every future Claude conversation more effective.</p>\n<p>Meta-learning: using AI to improve how you work with AI. That’s leverage.</p>",{headings:273,localImagePaths:274,remoteImagePaths:275,frontmatter:276,imagePaths:281},[],[],[],{title:265,date:277,tags:278},["Date","2025-12-03T00:00:00.000Z"],[166,279,168,280,169],"meta-learning","Claude Opus",[],"2025-12-03-refactoring-claude-md.md","2025-12-06-meta-review",{id:283,data:285,body:289,filePath:290,digest:291,rendered:292,legacyId:304},{title:286,date:287,type:42,description:288},"Meta-Review: Terminal 2.1 Spec Quality",["Date","2025-12-06T00:00:00.000Z"],"Using AI to review your own methodology is meta-learning at its finest.","After building the Terminal 2.1 spec, I turned Claude on myself. \"Review this spec against best practices - Spec-Driven Development, TDD guidelines, Hermeneutic Circle methodology. How does it hold up?\"\r\n\r\nThe results were humbling. Strong marks for Hub-Spoke Architecture ✅, 600-line rule compliance ✅, and UdonSharp checks ✅. But big gaps: no TDD integration ❌, missing Hermeneutic Circle analysis ❌, incomplete pre-commit workflow ❌.\r\n\r\n<a href=\"/images/2025/12/terminal-spec-review-dec6.png\" target=\"_blank\">\r\n    <img src=\"/images/2025/12/terminal-spec-review-dec6.png\" alt=\"Alignment analysis showing Terminal 2.1 spec strengths and gaps\" style=\"max-width: 50%; border: 1px solid var(--border-color); margin: 10px 0; cursor: pointer;\" />\r\n</a>\r\n\r\nThis is how you get better - critique your own work with the same rigor you'd apply to someone else's. The spec demonstrates solid architecture thinking, but I'm not validating it with tests or considering WHOLE ↔ PART impacts explicitly. Those are fixable gaps.\r\n\r\nUsing AI to review your own methodology is meta-learning at its finest. It's not about getting praise - it's about finding the blind spots.","src/content/devlog/2025-12-06-meta-review.md","fd4be10c5c693fb3",{html:293,metadata:294},"<p>After building the Terminal 2.1 spec, I turned Claude on myself. “Review this spec against best practices - Spec-Driven Development, TDD guidelines, Hermeneutic Circle methodology. How does it hold up?”</p>\n<p>The results were humbling. Strong marks for Hub-Spoke Architecture ✅, 600-line rule compliance ✅, and UdonSharp checks ✅. But big gaps: no TDD integration ❌, missing Hermeneutic Circle analysis ❌, incomplete pre-commit workflow ❌.</p>\n<a href=\"/images/2025/12/terminal-spec-review-dec6.png\" target=\"_blank\">\n    <img src=\"/images/2025/12/terminal-spec-review-dec6.png\" alt=\"Alignment analysis showing Terminal 2.1 spec strengths and gaps\" style=\"max-width: 50%; border: 1px solid var(--border-color); margin: 10px 0; cursor: pointer;\">\n</a>\n<p>This is how you get better - critique your own work with the same rigor you’d apply to someone else’s. The spec demonstrates solid architecture thinking, but I’m not validating it with tests or considering WHOLE ↔ PART impacts explicitly. Those are fixable gaps.</p>\n<p>Using AI to review your own methodology is meta-learning at its finest. It’s not about getting praise - it’s about finding the blind spots.</p>",{headings:295,localImagePaths:296,remoteImagePaths:297,frontmatter:298,imagePaths:303},[],[],[],{title:286,date:299,type:42,tags:300,description:288},["Date","2025-12-06T00:00:00.000Z"],[279,166,301,302,280],"methodology","Spec-Driven Development",[],"2025-12-06-meta-review.md","2025-12-11-devlog-system-simplification",{id:305,data:307,body:310,filePath:311,digest:312,rendered:313,legacyId:337},{title:308,date:309,type:42},"Devlog System Simplification Analysis",["Date","2025-12-11T00:00:00.000Z"],"### The Problem: Over-Engineering the Documentation\r\nI realized that my initial plan for the Automated Devlog System was becoming a project in itself. The original design involved:\r\n- **3 different templates**\r\n- **Automated impact scoring algorithms**\r\n- **4 separate Python scripts**\r\n- **AI \"guessing\" why things mattered**\r\n\r\nIt was estimated to take **8-11 days** to build. That's too much overhead for a system meant to *save* time.\r\n\r\n### The Solution: 90% Simplification\r\nI re-evaluated the requirements against the core mission: **chronicling the AI skill journey**. I realized that the developer (me) always knows *what* matters—I just need help structuring it.\r\n\r\n**The New \"Lite\" Workflow:**\r\n1. **One Master Template:** No more auto-classification logic. I pick the type ([Milestone], [TIL], [Meta]).\r\n2. **Dialogue > Algorithms:** Instead of predicting importance, the system will just ask me: *\"What's your one-liner takeaway?\"* and *\"Why does this matter?\"*.\r\n3. **AI Synthesis:** The agent takes my raw reflection and structures it into the narrative format.\r\n\r\n### Why It Matters\r\nThis reduces the build time from **two weeks** to **~1 day**. \r\n\r\nIt shifts the focus from building complex logic to **capturing authentic learning moments**. By replacing \"AI guessing\" with \"Human reflection,\" the devlogs will be more insightful and personal, while still leveraging AI for the heavy lifting of formatting and publishing.\r\n\r\n**Key Insight:** Automation shouldn't replace the *thinking*—it should remove the friction of *documenting* that thinking.","src/content/devlog/2025-12-11-devlog-system-simplification.md","6218580aaecd2533",{html:314,metadata:315},"<h3 id=\"the-problem-over-engineering-the-documentation\">The Problem: Over-Engineering the Documentation</h3>\n<p>I realized that my initial plan for the Automated Devlog System was becoming a project in itself. The original design involved:</p>\n<ul>\n<li><strong>3 different templates</strong></li>\n<li><strong>Automated impact scoring algorithms</strong></li>\n<li><strong>4 separate Python scripts</strong></li>\n<li><strong>AI “guessing” why things mattered</strong></li>\n</ul>\n<p>It was estimated to take <strong>8-11 days</strong> to build. That’s too much overhead for a system meant to <em>save</em> time.</p>\n<h3 id=\"the-solution-90-simplification\">The Solution: 90% Simplification</h3>\n<p>I re-evaluated the requirements against the core mission: <strong>chronicling the AI skill journey</strong>. I realized that the developer (me) always knows <em>what</em> matters—I just need help structuring it.</p>\n<p><strong>The New “Lite” Workflow:</strong></p>\n<ol>\n<li><strong>One Master Template:</strong> No more auto-classification logic. I pick the type ([Milestone], [TIL], [Meta]).</li>\n<li><strong>Dialogue > Algorithms:</strong> Instead of predicting importance, the system will just ask me: <em>“What’s your one-liner takeaway?”</em> and <em>“Why does this matter?”</em>.</li>\n<li><strong>AI Synthesis:</strong> The agent takes my raw reflection and structures it into the narrative format.</li>\n</ol>\n<h3 id=\"why-it-matters\">Why It Matters</h3>\n<p>This reduces the build time from <strong>two weeks</strong> to <strong>~1 day</strong>.</p>\n<p>It shifts the focus from building complex logic to <strong>capturing authentic learning moments</strong>. By replacing “AI guessing” with “Human reflection,” the devlogs will be more insightful and personal, while still leveraging AI for the heavy lifting of formatting and publishing.</p>\n<p><strong>Key Insight:</strong> Automation shouldn’t replace the <em>thinking</em>—it should remove the friction of <em>documenting</em> that thinking.</p>",{headings:316,localImagePaths:327,remoteImagePaths:328,frontmatter:329,imagePaths:336},[317,321,324],{depth:318,slug:319,text:320},3,"the-problem-over-engineering-the-documentation","The Problem: Over-Engineering the Documentation",{depth:318,slug:322,text:323},"the-solution-90-simplification","The Solution: 90% Simplification",{depth:318,slug:325,text:326},"why-it-matters","Why It Matters",[],[],{title:308,date:330,tags:331},["Date","2025-12-11T00:00:00.000Z"],[332,167,333,334,335],"planning","ai-workflow","efficiency","meta",[],"2025-12-11-devlog-system-simplification.md","2025-12-20-taking-control-my-nas-setup",{id:338,data:340,body:344,filePath:345,digest:346,rendered:347,legacyId:356},{title:341,date:342,type:42,description:343},"Taking Control: My NAS Setup",["Date","2025-12-20T00:00:00.000Z"],"Standing up my first NAS and taking ownership of my digital life.","I’m migrating my data off cloud services (Google Drive, Dropbox, Google Photos) to local storage.\r\n\r\n<img width=\"512\" height=\"343\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/29eaae8e-f354-473a-8025-0c98b2838a53\" />\r\n\r\n**The Foundation:**\r\n\r\n- Synology DS224+ NAS\r\n- 2×8TB drives in RAID 1 (mirrored for redundancy)\r\n- 8GB RAM\r\n- Running Immich for our 21-year, 85K+ photo library\r\n\r\nCloud storage costs $10-20/month indefinitely. A NAS is a one-time hardware cost with no monthly fees, and my data stays completely under my control—no third-party access, processing, or terms-of-service changes.\r\n\r\nHosting my own data means I'm not limited to any one platform for file and photo hosting.  This opens up a lot of new exciting opportunities for exploring open source solutions.\r\n\r\nOne specific example is migration from Google Photos to Immich. \r\n\r\nGoogle has a habit of changing their website interfaces (from Picasa, to Google Photos and  Google Music to YouTube Music) and they always seem to be lacking features to previous interface had.\r\n\r\nOpen source software Immich replicates Google Photos functionality (facial recognition, search, automatic backups) on my own hardware. I keep the convenience, lose the ongoing costs and privacy trade-offs.\r\n\r\nUsed googles Takeout service and then Immich-go for the import process.  When I wake up I will have a new photo library to enjoy.","src/content/devlog/2025-12-20-taking-control-my-nas-setup.md","19ce8f943b38793f",{html:348,metadata:349},"<p>I’m migrating my data off cloud services (Google Drive, Dropbox, Google Photos) to local storage.</p>\n<img width=\"512\" height=\"343\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/29eaae8e-f354-473a-8025-0c98b2838a53\">\n<p><strong>The Foundation:</strong></p>\n<ul>\n<li>Synology DS224+ NAS</li>\n<li>2×8TB drives in RAID 1 (mirrored for redundancy)</li>\n<li>8GB RAM</li>\n<li>Running Immich for our 21-year, 85K+ photo library</li>\n</ul>\n<p>Cloud storage costs $10-20/month indefinitely. A NAS is a one-time hardware cost with no monthly fees, and my data stays completely under my control—no third-party access, processing, or terms-of-service changes.</p>\n<p>Hosting my own data means I’m not limited to any one platform for file and photo hosting.  This opens up a lot of new exciting opportunities for exploring open source solutions.</p>\n<p>One specific example is migration from Google Photos to Immich.</p>\n<p>Google has a habit of changing their website interfaces (from Picasa, to Google Photos and  Google Music to YouTube Music) and they always seem to be lacking features to previous interface had.</p>\n<p>Open source software Immich replicates Google Photos functionality (facial recognition, search, automatic backups) on my own hardware. I keep the convenience, lose the ongoing costs and privacy trade-offs.</p>\n<p>Used googles Takeout service and then Immich-go for the import process.  When I wake up I will have a new photo library to enjoy.</p>",{headings:350,localImagePaths:351,remoteImagePaths:352,frontmatter:353,imagePaths:355},[],[],[],{title:341,date:354,type:42,description:343},["Date","2025-12-20T00:00:00.000Z"],[],"2025-12-20-taking-control-my-nas-setup.md","2025-12-29-introducing-bbp-ai-issue-prioritization",{id:357,data:359,body:362,filePath:363,digest:364,rendered:365,legacyId:387},{title:360,date:361,type:42},"Introducing BBP: An AI-Driven Issue Prioritization System",["Date","2025-12-29T00:00:00.000Z"],"After successfully having Claude Code autonomously implement `music.exe`—a Basement OS terminal app that integrates with ProTV 3.x for real-time playlist browsing and playback control—I wanted to scale that approach. Instead of picking issues randomly or by gut feeling, what if AI could act as a SCRUM master and pre-spec *everything*?\r\n\r\n### The Challenge\r\n\r\nWith 58 open issues across features, bugs, concepts, and epics, there was no clear way to know:\r\n- Which issues Claude Code could handle autonomously\r\n- Which had the highest \"basement nostalgia\" impact\r\n- How to balance effort vs. payoff\r\n\r\nI needed a system that would lay out work explicitly, so I could return and start building immediately.\r\n\r\n### The Solution\r\n\r\nI created **Basement Build Priority (BBP)**—a scoring formula:\r\n\r\n```\r\nBBP = (Agentic_Feasibility × Nostalgia_Score) / Story_Points\r\n```\r\n\r\n| Metric | Range | Purpose |\r\n|--------|-------|---------|\r\n| Agentic Feasibility | 0-100% | Can Claude + Unity MCP complete this? |\r\n| Nostalgia Score | 1-10 | Does it make the basement feel alive? |\r\n| Story Points | 1-21 | Fibonacci effort scale |\r\n\r\nClaude analyzed all 58 issues, assigned scores, and applied a **\"Good Agentic Build\"** label to 36 issues with ≥70% feasibility. The result: a prioritized backlog where high-BBP items are high-automation, high-nostalgia, low-effort wins.\r\n\r\n### Why It Matters\r\n\r\nThis shifts Claude from \"assistant\" to \"project manager.\" Instead of asking \"what should I work on?\", the backlog is pre-scored and ready. If the Agentic + MCP combo continues to prove itself (like it did with music.exe), it will take over a meaningful portion of the workload—in theory I could assign Claude to work on agentic issues while I focus on the non-agentic ones. My bandwidth is now \"monitoring and review\" rather than code, integrate, test, verify.\r\n\r\n### The Paradigm Shift\r\n\r\nThis represents the third evolution in my AI journey:\r\n\r\n| Phase | Mindset | Limiting Factor |\r\n|-------|---------|-----------------|\r\n| **Before AI** | \"What *can* I build?\" | Skill |\r\n| **With AI** | \"What *should* I build?\" | Imagination |\r\n| **Agentic AI** | \"What will Claude build while I review?\" | Bandwidth |\r\n\r\n**Key Insight:** Pre-scoring issues with AI as SCRUM master (and reviewing its accuracy) means when I dedicate time to build, I can put Claude to work on low-impact agentic issues while I tackle the highest-impact ones—and have a head start. It could in theory complete an issue 60% of the way if it has 60% agentic feasibility, and I only have to finish the last 40% instead of 100%.","src/content/devlog/2025-12-29-introducing-bbp-ai-issue-prioritization.md","87aee21c1079da2d",{html:366,metadata:367},"<p>After successfully having Claude Code autonomously implement <code>music.exe</code>—a Basement OS terminal app that integrates with ProTV 3.x for real-time playlist browsing and playback control—I wanted to scale that approach. Instead of picking issues randomly or by gut feeling, what if AI could act as a SCRUM master and pre-spec <em>everything</em>?</p>\n<h3 id=\"the-challenge\">The Challenge</h3>\n<p>With 58 open issues across features, bugs, concepts, and epics, there was no clear way to know:</p>\n<ul>\n<li>Which issues Claude Code could handle autonomously</li>\n<li>Which had the highest “basement nostalgia” impact</li>\n<li>How to balance effort vs. payoff</li>\n</ul>\n<p>I needed a system that would lay out work explicitly, so I could return and start building immediately.</p>\n<h3 id=\"the-solution\">The Solution</h3>\n<p>I created <strong>Basement Build Priority (BBP)</strong>—a scoring formula:</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>BBP = (Agentic_Feasibility × Nostalgia_Score) / Story_Points</span></span></code></pre>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Metric</th><th>Range</th><th>Purpose</th></tr></thead><tbody><tr><td>Agentic Feasibility</td><td>0-100%</td><td>Can Claude + Unity MCP complete this?</td></tr><tr><td>Nostalgia Score</td><td>1-10</td><td>Does it make the basement feel alive?</td></tr><tr><td>Story Points</td><td>1-21</td><td>Fibonacci effort scale</td></tr></tbody></table>\n<p>Claude analyzed all 58 issues, assigned scores, and applied a <strong>“Good Agentic Build”</strong> label to 36 issues with ≥70% feasibility. The result: a prioritized backlog where high-BBP items are high-automation, high-nostalgia, low-effort wins.</p>\n<h3 id=\"why-it-matters\">Why It Matters</h3>\n<p>This shifts Claude from “assistant” to “project manager.” Instead of asking “what should I work on?”, the backlog is pre-scored and ready. If the Agentic + MCP combo continues to prove itself (like it did with music.exe), it will take over a meaningful portion of the workload—in theory I could assign Claude to work on agentic issues while I focus on the non-agentic ones. My bandwidth is now “monitoring and review” rather than code, integrate, test, verify.</p>\n<h3 id=\"the-paradigm-shift\">The Paradigm Shift</h3>\n<p>This represents the third evolution in my AI journey:</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Phase</th><th>Mindset</th><th>Limiting Factor</th></tr></thead><tbody><tr><td><strong>Before AI</strong></td><td>”What <em>can</em> I build?”</td><td>Skill</td></tr><tr><td><strong>With AI</strong></td><td>”What <em>should</em> I build?”</td><td>Imagination</td></tr><tr><td><strong>Agentic AI</strong></td><td>”What will Claude build while I review?”</td><td>Bandwidth</td></tr></tbody></table>\n<p><strong>Key Insight:</strong> Pre-scoring issues with AI as SCRUM master (and reviewing its accuracy) means when I dedicate time to build, I can put Claude to work on low-impact agentic issues while I tackle the highest-impact ones—and have a head start. It could in theory complete an issue 60% of the way if it has 60% agentic feasibility, and I only have to finish the last 40% instead of 100%.</p>",{headings:368,localImagePaths:379,remoteImagePaths:380,frontmatter:381,imagePaths:386},[369,372,375,376],{depth:318,slug:370,text:371},"the-challenge","The Challenge",{depth:318,slug:373,text:374},"the-solution","The Solution",{depth:318,slug:325,text:326},{depth:318,slug:377,text:378},"the-paradigm-shift","The Paradigm Shift",[],[],{title:360,date:382,tags:383},["Date","2025-12-29T00:00:00.000Z"],[167,168,384,385,335],"workflow","project-management",[],"2025-12-29-introducing-bbp-ai-issue-prioritization.md","2025-12-26-music-exe-ai-coded-protv-integration",{id:388,data:390,body:393,filePath:394,digest:395,rendered:396,legacyId:409},{title:391,date:392,type:16},"MUSIC.EXE: 90% AI-Coded ProTV Integration for Basement OS",["Date","2025-12-26T00:00:00.000Z"],"The default ProTV playlist UI worked, but doesn't match the asthetic of Lower Level 2.0, which is realism and nostalgia. I wanted a terminal-native music player that matched the DOS aesthetic and could be navigated with keyboard or joystick controls. Enter **MUSIC.EXE**: a fully functional ProTV music player app, coded 90% by AI with my guidance.\r\n\r\n![Before: The default ProTV playlist UI isfunctional but out of place in the Lower Level 2.0 aesthetic](/images/devlog/throwback-mix-before.png)\r\n\r\n\r\n### The Challenge\r\n\r\nThe real challenge wasn't coding, it was picking a task that AI could actually *accomplish* with its \"hands and eyes.\" This was the first real test of my [Full Stack AI Workflow](/devlog#entry-2025-12-07-full-stack-ai) architecture: could Claude Code, equipped with Unity MCP tools and custom Editor scripts, autonomously implement a complete feature?\r\n\r\nProTV integration was the perfect candidate:\r\n- **Well-documented API** ([ProTV 3.x Documentation](https://protv.dev/news/protv3-beta))\r\n- **Clear input/output patterns** (IN_/OUT_ variable injection)\r\n- **Isolated scope** (one app, one integration point)\r\n\r\nThe key was creating a **multi-layer prompt** that gave Claude the domain expertise it needed. Rather than hoping it would figure out ProTV's non-standard APIs, I front-loaded the knowledge:\r\n\r\n> You are an expert ProTV 3.x integration specialist for VRChat UdonSharp development. You understand the critical differences between **event-driven** and **polling-based** integration patterns, and you know the exact APIs, variable conventions, and pitfalls of ProTV's plugin architecture.\r\n>\r\n> **CRITICAL RULE: NEVER GUESS.** If you don't know an API or are uncertain about ProTV behavior, read the ProTV source, check existing implementations, or ask for clarification. **DO NOT hallucinate ProTV APIs.**\r\n\r\n### The Solution\r\n\r\nThe development spanned December 17-26, 2025, across three sessions:\r\n\r\n**Session 1 (Dec 17)**: Initial implementation 497 lines of C# for playlist browsing, track navigation, and playback control. Code compiled, but Claude hit a wall: Unity MCP tools couldn't set object references in the Inspector.\r\n\r\n**Session 2 (Dec 25)**: The breakthrough. Instead of declaring \"manual intervention required,\" Claude remembered the project's prime directive: *\"If you get stuck, can you resolve the roadblock with a Unity Editor script?\"* It expanded `SetupDTAppMusic.cs` to handle all wiring autonomously with no Inspector clicks needed.\r\n\r\n**Session 3 (Dec 26)**: Final integration. Converted from polling-based to event-driven ProTV integration, fixed the `sortView` shuffle index mapping, and verified end-to-end playback.\r\n\r\nThe result: **473 lines of production code**, plus Editor automation, delivered with ~10% human intervention (mostly debugging ProTV's undocumented `sortView` behavior).\r\n\r\n### Why It Matters\r\n\r\nThis proves the viability of **full closed-loop autonomous development** for non-trivial features:\r\n\r\n1. **AI as Workflow Architect** — The 90/10 split is real. AI handles the bulk of implementation while I focus on architecture decisions, debugging edge cases, and validation.\r\n\r\n2. **Reusable Agent Patterns** — The ProTV prompt I created isn't throwaway. It becomes a [reusable agent/skill](/skills#protv-integration) for future ProTV integrations. Each solved problem compounds into institutional knowledge.\r\n\r\n3. **Scalable Approach** — If MUSIC.EXE works, the same pattern applies to other Basement OS apps: identify scope, create domain-specific prompts, let Claude execute.\r\n\r\n**Key Insight:** AI might not achieve 100%, but if it consistently delivers 90%, I only need to contribute the remaining 10%. That's a 10x multiplier on my development capacity.\r\n\r\n![After: MUSIC.EXE running in Basement OS—terminal-native playlist browser with keyboard navigation](/images/devlog/music-exe-after.jpg)","src/content/devlog/2025-12-26-music-exe-ai-coded-protv-integration.md","2ce3b7b48630e7ae",{html:397,metadata:398},"<p>The default ProTV playlist UI worked, but doesn’t match the asthetic of Lower Level 2.0, which is realism and nostalgia. I wanted a terminal-native music player that matched the DOS aesthetic and could be navigated with keyboard or joystick controls. Enter <strong>MUSIC.EXE</strong>: a fully functional ProTV music player app, coded 90% by AI with my guidance.</p>\n<p><img src=\"/images/devlog/throwback-mix-before.png\" alt=\"Before: The default ProTV playlist UI isfunctional but out of place in the Lower Level 2.0 aesthetic\"></p>\n<h3 id=\"the-challenge\">The Challenge</h3>\n<p>The real challenge wasn’t coding, it was picking a task that AI could actually <em>accomplish</em> with its “hands and eyes.” This was the first real test of my <a href=\"/devlog#entry-2025-12-07-full-stack-ai\">Full Stack AI Workflow</a> architecture: could Claude Code, equipped with Unity MCP tools and custom Editor scripts, autonomously implement a complete feature?</p>\n<p>ProTV integration was the perfect candidate:</p>\n<ul>\n<li><strong>Well-documented API</strong> (<a href=\"https://protv.dev/news/protv3-beta\">ProTV 3.x Documentation</a>)</li>\n<li><strong>Clear input/output patterns</strong> (IN_/OUT_ variable injection)</li>\n<li><strong>Isolated scope</strong> (one app, one integration point)</li>\n</ul>\n<p>The key was creating a <strong>multi-layer prompt</strong> that gave Claude the domain expertise it needed. Rather than hoping it would figure out ProTV’s non-standard APIs, I front-loaded the knowledge:</p>\n<blockquote>\n<p>You are an expert ProTV 3.x integration specialist for VRChat UdonSharp development. You understand the critical differences between <strong>event-driven</strong> and <strong>polling-based</strong> integration patterns, and you know the exact APIs, variable conventions, and pitfalls of ProTV’s plugin architecture.</p>\n<p><strong>CRITICAL RULE: NEVER GUESS.</strong> If you don’t know an API or are uncertain about ProTV behavior, read the ProTV source, check existing implementations, or ask for clarification. <strong>DO NOT hallucinate ProTV APIs.</strong></p>\n</blockquote>\n<h3 id=\"the-solution\">The Solution</h3>\n<p>The development spanned December 17-26, 2025, across three sessions:</p>\n<p><strong>Session 1 (Dec 17)</strong>: Initial implementation 497 lines of C# for playlist browsing, track navigation, and playback control. Code compiled, but Claude hit a wall: Unity MCP tools couldn’t set object references in the Inspector.</p>\n<p><strong>Session 2 (Dec 25)</strong>: The breakthrough. Instead of declaring “manual intervention required,” Claude remembered the project’s prime directive: <em>“If you get stuck, can you resolve the roadblock with a Unity Editor script?”</em> It expanded <code>SetupDTAppMusic.cs</code> to handle all wiring autonomously with no Inspector clicks needed.</p>\n<p><strong>Session 3 (Dec 26)</strong>: Final integration. Converted from polling-based to event-driven ProTV integration, fixed the <code>sortView</code> shuffle index mapping, and verified end-to-end playback.</p>\n<p>The result: <strong>473 lines of production code</strong>, plus Editor automation, delivered with ~10% human intervention (mostly debugging ProTV’s undocumented <code>sortView</code> behavior).</p>\n<h3 id=\"why-it-matters\">Why It Matters</h3>\n<p>This proves the viability of <strong>full closed-loop autonomous development</strong> for non-trivial features:</p>\n<ol>\n<li>\n<p><strong>AI as Workflow Architect</strong> — The 90/10 split is real. AI handles the bulk of implementation while I focus on architecture decisions, debugging edge cases, and validation.</p>\n</li>\n<li>\n<p><strong>Reusable Agent Patterns</strong> — The ProTV prompt I created isn’t throwaway. It becomes a <a href=\"/skills#protv-integration\">reusable agent/skill</a> for future ProTV integrations. Each solved problem compounds into institutional knowledge.</p>\n</li>\n<li>\n<p><strong>Scalable Approach</strong> — If MUSIC.EXE works, the same pattern applies to other Basement OS apps: identify scope, create domain-specific prompts, let Claude execute.</p>\n</li>\n</ol>\n<p><strong>Key Insight:</strong> AI might not achieve 100%, but if it consistently delivers 90%, I only need to contribute the remaining 10%. That’s a 10x multiplier on my development capacity.</p>\n<p><img src=\"/images/devlog/music-exe-after.jpg\" alt=\"After: MUSIC.EXE running in Basement OS—terminal-native playlist browser with keyboard navigation\"></p>",{headings:399,localImagePaths:403,remoteImagePaths:404,frontmatter:405,imagePaths:408},[400,401,402],{depth:318,slug:370,text:371},{depth:318,slug:373,text:374},{depth:318,slug:325,text:326},[],[],{title:391,date:406,tags:407,type:16},["Date","2025-12-26T00:00:00.000Z"],[167,168,31,32,16],[],"2025-12-26-music-exe-ai-coded-protv-integration.md","2025-12-07-full-stack-ai",{id:410,data:412,body:416,filePath:417,digest:418,rendered:419,legacyId:430},{title:413,date:414,type:16,description:415},"Full Stack AI Workflow - The Complete System",["Date","2025-12-07T00:00:00.000Z"],"Two breakthroughs in one day. This is the moment everything clicked.","Two breakthroughs in one day. This is the moment everything clicked.\r\n\r\n**Morning - The Assembly Line:** Launched my first <span class=\"term\" tabindex=\"0\">agent swarm<span class=\"tooltip\">Multiple AI agents working in parallel on different tasks simultaneously</span></span> with custom `agent.md` persona files. Nine specialized agents building different Basement OS modules simultaneously - DT_Core, DT_Shell, DT_Theme, weather app, GitHub app, each with injected expertise. This is horizontal scaling - volume without sacrificing architecture.\r\n\r\n<a href=\"/images/2025/12/agent-swarm-first-time-dec7.png\" target=\"_blank\">\r\n    <img src=\"/images/2025/12/agent-swarm-first-time-dec7.png\" alt=\"First agent swarm with 9 parallel agents and custom agent.md files\" style=\"max-width: 50%; border: 1px solid var(--border-color); margin: 10px 0; cursor: pointer;\" />\r\n</a>\r\n\r\n**The Gap:** Agents could write perfect code, but Unity wouldn't compile it. Files sat on disk, ignored. I was still manually clicking \"Compile\" in the Inspector. The automation loop was broken.\r\n\r\n**Afternoon - The Missing Link:** Found `UdonSharpAssetRepair.cs` - the linchpin I'd been missing. This utility script forces Unity to acknowledge programmatically-written files, generates the .asset files, and triggers compilation. It's the bridge between \"AI writes code\" and \"Unity actually runs it.\"\r\n\r\n<a href=\"/images/2025/12/full-automation-udonsharp-asset-repair-dec7.png\" target=\"_blank\">\r\n    <img src=\"/images/2025/12/full-automation-udonsharp-asset-repair-dec7.png\" alt=\"Full automation achieved with UdonSharpAssetRepair as the missing link\" style=\"max-width: 50%; border: 1px solid var(--border-color); margin: 10px 0; cursor: pointer;\" />\r\n</a>\r\n\r\n<a href=\"/images/2025/12/autonomous-agent-workflow-dec7.png\" target=\"_blank\">\r\n    <img src=\"/images/2025/12/autonomous-agent-workflow-dec7.png\" alt=\"Creating comprehensive SOP documentation for autonomous agent workflow\" style=\"max-width: 50%; border: 1px solid var(--border-color); margin: 10px 0; cursor: pointer;\" />\r\n</a>\r\n\r\n<a href=\"/images/2025/12/closed-loop-complete-dec7.png\" target=\"_blank\">\r\n    <img src=\"/images/2025/12/closed-loop-complete-dec7.png\" alt=\"Documentation complete - CLOSED_LOOP_AGENT_SYSTEM.md and system files\" style=\"max-width: 50%; border: 1px solid var(--border-color); margin: 10px 0; cursor: pointer;\" />\r\n</a>\r\n\r\n**The Complete System:** Now Claude writes code → triggers UdonSharpAssetRepair → Unity compiles → enters Play mode → reads console logs → fixes errors → repeats. Zero human intervention. The swarm builds the car fast (volume). The automation pipeline ensures it doesn't explode when you turn the key (quality assurance).\r\n\r\n**The Learning:** This is tooling engineering. The swarm was impressive, but useless without the testing loop. UdonSharpAssetRepair is a 200-line script that unlocks millions of dollars in productivity. Finding these linchpins - the small pieces that complete the system - that's the skill companies pay for.\r\n\r\n**Why It Matters:** I went from \"AI-assisted developer\" to \"AI workflow architect.\" The difference? I'm not just using tools - I'm building the automation that makes the tools useful. That's the career transition I'm chasing.","src/content/devlog/2025-12-07-full-stack-ai.md","23635fe6299f1a63",{html:420,metadata:421},"<p>Two breakthroughs in one day. This is the moment everything clicked.</p>\n<p><strong>Morning - The Assembly Line:</strong> Launched my first <span class=\"term\" tabindex=\"0\">agent swarm<span class=\"tooltip\">Multiple AI agents working in parallel on different tasks simultaneously</span></span> with custom <code>agent.md</code> persona files. Nine specialized agents building different Basement OS modules simultaneously - DT_Core, DT_Shell, DT_Theme, weather app, GitHub app, each with injected expertise. This is horizontal scaling - volume without sacrificing architecture.</p>\n<a href=\"/images/2025/12/agent-swarm-first-time-dec7.png\" target=\"_blank\">\n    <img src=\"/images/2025/12/agent-swarm-first-time-dec7.png\" alt=\"First agent swarm with 9 parallel agents and custom agent.md files\" style=\"max-width: 50%; border: 1px solid var(--border-color); margin: 10px 0; cursor: pointer;\">\n</a>\n<p><strong>The Gap:</strong> Agents could write perfect code, but Unity wouldn’t compile it. Files sat on disk, ignored. I was still manually clicking “Compile” in the Inspector. The automation loop was broken.</p>\n<p><strong>Afternoon - The Missing Link:</strong> Found <code>UdonSharpAssetRepair.cs</code> - the linchpin I’d been missing. This utility script forces Unity to acknowledge programmatically-written files, generates the .asset files, and triggers compilation. It’s the bridge between “AI writes code” and “Unity actually runs it.”</p>\n<a href=\"/images/2025/12/full-automation-udonsharp-asset-repair-dec7.png\" target=\"_blank\">\n    <img src=\"/images/2025/12/full-automation-udonsharp-asset-repair-dec7.png\" alt=\"Full automation achieved with UdonSharpAssetRepair as the missing link\" style=\"max-width: 50%; border: 1px solid var(--border-color); margin: 10px 0; cursor: pointer;\">\n</a>\n<a href=\"/images/2025/12/autonomous-agent-workflow-dec7.png\" target=\"_blank\">\n    <img src=\"/images/2025/12/autonomous-agent-workflow-dec7.png\" alt=\"Creating comprehensive SOP documentation for autonomous agent workflow\" style=\"max-width: 50%; border: 1px solid var(--border-color); margin: 10px 0; cursor: pointer;\">\n</a>\n<a href=\"/images/2025/12/closed-loop-complete-dec7.png\" target=\"_blank\">\n    <img src=\"/images/2025/12/closed-loop-complete-dec7.png\" alt=\"Documentation complete - CLOSED_LOOP_AGENT_SYSTEM.md and system files\" style=\"max-width: 50%; border: 1px solid var(--border-color); margin: 10px 0; cursor: pointer;\">\n</a>\n<p><strong>The Complete System:</strong> Now Claude writes code → triggers UdonSharpAssetRepair → Unity compiles → enters Play mode → reads console logs → fixes errors → repeats. Zero human intervention. The swarm builds the car fast (volume). The automation pipeline ensures it doesn’t explode when you turn the key (quality assurance).</p>\n<p><strong>The Learning:</strong> This is tooling engineering. The swarm was impressive, but useless without the testing loop. UdonSharpAssetRepair is a 200-line script that unlocks millions of dollars in productivity. Finding these linchpins - the small pieces that complete the system - that’s the skill companies pay for.</p>\n<p><strong>Why It Matters:</strong> I went from “AI-assisted developer” to “AI workflow architect.” The difference? I’m not just using tools - I’m building the automation that makes the tools useful. That’s the career transition I’m chasing.</p>",{headings:422,localImagePaths:423,remoteImagePaths:424,frontmatter:425,imagePaths:429},[],[],[],{title:413,date:426,type:16,tags:427,description:415},["Date","2025-12-07T00:00:00.000Z"],[29,167,257,168,259,258,428,169],"UdonSharpAssetRepair",[],"2025-12-07-full-stack-ai.md","devlog_temp",["Map",410,433],{id:410,data:434,body:416,filePath:438,digest:439,rendered:440,legacyId:430},{layout:435,title:413,date:436,type:16,tags:437,description:415},"../../layouts/DevlogEntry.astro",["Date","2025-12-07T00:00:00.000Z"],[29,167,257,168,259,258,428,169],"src/content/devlog_temp/2025-12-07-full-stack-ai.md","f27963ca20b53ac9",{html:420,metadata:441},{headings:442,localImagePaths:443,remoteImagePaths:444,frontmatter:434,imagePaths:445},[],[],[],[]];

export { _astro_dataLayerContent as default };
